{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK 자연어 처리 패키지 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형태소 분석 해주는 라이브러리가 NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK(Natural Language Toolkit) 패키지는 교육용으로 개발된 자연어 처리 및 문서 분석용 파이썬 패키지다. 다양한 기능 및 예제를 가지고 있으며 실무 및 연구에서도 많이 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK 패키지가 제공하는 주요 기능은 다음과 같다.\n",
    "\n",
    "* 샘플 corpus 및 사전\n",
    "* 토큰 생성(tokenizing)\n",
    "* 형태소 분석(stemming/lemmatizing)\n",
    "* 품사 태깅(part-of-speech tagging)\n",
    "* 구문 분석(syntax parsing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 샘플 corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 샘플 corpus 및 사전\n",
    "* 우리말로 말뭉치. 트레이닝 데이터\n",
    "* corpus는 여러 가지가 있는데 그 중의 일부만 리스트업을 해두었다.\n",
    "* 태깅은 사람이 해야 한다. 태깅 정보가 다 들어 있다. 워낙 양이 많다보니 하나하나 직접 설치를 해보았다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corpus는 분석 작업을 위한 샘플 문서 집합을 말한다. 단순히 소설, 신문 등의 문서를 모아놓은 것도 있지만 대부분 품사. 형태소, 등의 보조적 의미를 추가하고 쉬운 분석을 위해 구조적인 형태로 정리해 놓은 것이 많다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK 패키지의 corpus 서브패키지에서는 다음과 같은 다양한 연구용 corpus를 제공한다. 이 목록은 전체 corpus의 일부일 뿐이다.\n",
    "\n",
    "* averaged_perceptron_tagger Averaged Perceptron Tagger\n",
    "* book_grammars:  Grammars from NLTK Book\n",
    "* brown:  Brown Corpus\n",
    "* chat80:  Chat-80 Data Files\n",
    "* city_database:  City Database\n",
    "* comparative_sentences Comparative Sentence Dataset\n",
    "* dependency_treebank. Dependency Parsed Treebank\n",
    "* gutenberg:  Project Gutenberg Selections\n",
    "* hmm_treebank_pos_tagger Treebank Part of Speech Tagger (HMM)\n",
    "* inaugural:  C-Span Inaugural Address Corpus\n",
    "* large_grammars:  Large context-free and feature-based grammars for parser comparison\n",
    "* mac_morpho:  MAC-MORPHO: Brazilian Portuguese news text with part-of-speech tags\n",
    "* masc_tagged:  MASC Tagged Corpus\n",
    "* maxent_ne_chunker:  ACE Named Entity Chunker (Maximum entropy)\n",
    "* maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
    "* movie_reviews:  Sentiment Polarity Dataset Version 2.0\n",
    "* names:  Names Corpus, Version 1.3 (1994-03-29)\n",
    "* nps_chat:  NPS Chat\n",
    "* omw:  Open Multilingual Wordnet\n",
    "* opinion_lexicon:  Opinion Lexicon\n",
    "* pros_cons:  Pros and Cons\n",
    "* ptb:  Penn Treebank\n",
    "* punkt:  Punkt Tokenizer Models\n",
    "* reuters:  The Reuters-21578 benchmark corpus, ApteMod version\n",
    "* sample_grammars:  Sample Grammars\n",
    "* sentence_polarity:  Sentence Polarity Dataset v1.0\n",
    "* sentiwordnet:  SentiWordNet\n",
    "* snowball_data:  Snowball Data\n",
    "* stopwords:  Stopwords Corpus\n",
    "* subjectivity:  Subjectivity Dataset v1.0\n",
    "* tagsets:  Help on Tagsets\n",
    "* treebank:  Penn Treebank Sample\n",
    "* twitter_samples:  Twitter Samples\n",
    "* unicode_samples:  Unicode Samples\n",
    "* universal_tagset:  Mappings to the Universal Part-of-Speech Tagset\n",
    "* universal_treebanks_v20 Universal Treebanks Version 2.0\n",
    "* verbnet:  VerbNet Lexicon, Version 2.1\n",
    "* webtext:  Web Text Corpus\n",
    "* word2vec_sample:  Word2Vec Sample\n",
    "* wordnet:  WordNet\n",
    "* words:  Word Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 corpus 자료는 설치시에 제공되는 것이 아니라 `download` 명령으로 사용자가 다운로드 받아야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Error loading taggers: Package 'taggers' not found in\n",
      "[nltk_data]     index\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download(\"gutenberg\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('reuters')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"taggers\")\n",
    "nltk.download(\"webtext\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.  Her mother\n",
      "had died too long ago for her to have more than an indistinct\n",
      "remembrance of her caresses; and her place had been supplied\n",
      "by an excellent woman as governess, who had fallen little short\n",
      "of a mother in affection.\n",
      "\n",
      "Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\n",
      "less as a governess than a friend, very fond of both daughters,\n",
      "but particularly of Emma.  Between _them_ it was more the intimacy\n",
      "of sisters.  Even before Miss Taylor had ceased to hold the nominal\n",
      "office of governess, the mildness of her temper had hardly allowed\n",
      "her to impose any restraint; and the shadow of authority being\n",
      "now long passed away, they had been living together as friend and\n",
      "friend very mutually attached, and Emma doing just what she liked;\n",
      "highly esteeming Miss Taylor's judgment, but directed chiefly by\n",
      "her own.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emma_raw = nltk.corpus.gutenberg.raw(\"austen-emma.txt\")\n",
    "print(emma_raw[:1302])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰 생성(tokenizing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장을 쪼개는 것. 전체 텍스트를 쪼개는 과정. 어떻게 쪼개느냐? 그건 다 다르다. 자기가 원하는 방식으로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서를 분석하기 위해서는 우선 긴 문자열을 분석을 위한 작은 단위로 나누어야 한다. 이 문자열 단위를 토큰(token)이라고 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emma',\n",
       " 'Woodhouse',\n",
       " ',',\n",
       " 'handsome',\n",
       " ',',\n",
       " 'clever',\n",
       " ',',\n",
       " 'and',\n",
       " 'rich',\n",
       " ',',\n",
       " 'with',\n",
       " 'a']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(emma_raw[50:100])   #,도 별도로 token으로 만들어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emma', 'Woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "t = RegexpTokenizer(\"[\\w]+\")   #글자로 인식할 수 있는 거 1개 이상. ,는 생략\n",
    "t.tokenize(emma_raw[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\n",
      "less as a governess than a friend, very fond of both daughters,\n",
      "but particularly of Emma.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "print(sent_tokenize(emma_raw[:1000])[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형태소 분석이란 어근, 접두사/접미사, 품사(POS, part-of-speech) 등 다양한 언어적 속성의 구조를 파악하는 작업이다. 구체적으로는 다음과 같은 작업으로 나뉜다.\n",
    "\n",
    "* stemming (어근 추출)\n",
    "* lemmatizing (원형 복원)\n",
    "* POS tagging (품사 태깅)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Stemming and lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "st.stem(\"eating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shop'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "st = LancasterStemmer()\n",
    "st.stem(\"shopping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cook'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "st = RegexpStemmer(\"ing\")\n",
    "st.stem(\"cooking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking\n",
      "cook\n",
      "cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "print(lm.lemmatize(\"cooking\"))\n",
    "print(lm.lemmatize(\"cooking\", pos=\"v\"))\n",
    "print(lm.lemmatize(\"cookbooks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "belief\n",
      "believ\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize(\"believes\"))\n",
    "print(LancasterStemmer().stem(\"believes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS(part-of-speech)는 품사를 말한다. \n",
    "\n",
    "* Part-of-Speech Tagset\n",
    " * https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.htm\n",
    " * http://www.ibm.com/support/knowledgecenter/ko/SS5RWK_3.5.0/com.ibm.discovery.es.ta.doc/iiysspostagset.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[', 'NNS'),\n",
       " ('Emma', 'NNP'),\n",
       " ('by', 'IN'),\n",
       " ('Jane', 'NNP'),\n",
       " ('Austen', 'NNP'),\n",
       " ('1816', 'CD'),\n",
       " (']', 'NNP'),\n",
       " ('VOLUME', 'NNP'),\n",
       " ('I', 'PRP'),\n",
       " ('CHAPTER', 'VBP'),\n",
       " ('I', 'PRP'),\n",
       " ('Emma', 'NNP'),\n",
       " ('Woodhouse', 'NNP'),\n",
       " (',', ','),\n",
       " ('handsome', 'NN'),\n",
       " (',', ','),\n",
       " ('clever', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('rich', 'JJ'),\n",
       " (',', ','),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "tagged_list = pos_tag(word_tokenize(emma_raw[:100]))\n",
    "tagged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'Emma',\n",
       " 'by',\n",
       " 'Jane',\n",
       " 'Austen',\n",
       " '1816',\n",
       " ']',\n",
       " 'VOLUME',\n",
       " 'I',\n",
       " 'CHAPTER',\n",
       " 'I',\n",
       " 'Emma',\n",
       " 'Woodhouse',\n",
       " ',',\n",
       " 'handsome',\n",
       " ',',\n",
       " 'clever',\n",
       " ',',\n",
       " 'and',\n",
       " 'rich',\n",
       " ',',\n",
       " 'with',\n",
       " 'a']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import untag\n",
    "untag(tagged_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
