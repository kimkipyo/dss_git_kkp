{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문서 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 데이터 분석 모형은 숫자로 구성된 고정 차원 벡터를 독립 변수로 하고 있으므로 문서(document)를 분석을 하는 경우에도 숫자로 구성된 특징 벡터(feature vector)를 문서로부터 추출하는 과정이 필요하다. 이러한 과정을 문서 전처리(document preprocessing)라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW (Bag of Words)\n",
    "\n",
    "문서를 숫자 벡터로 변환하는 가장 기본적인 방법은 BOW (Bag of Words) 이다. BOW 방법에서는 전체 문서 $\\{D_1, D_2, \\ldots, D_n\\}$ 를 구성하는 고정된 단어장(vocabulary) $\\{W_1, W_2, \\ldots, W_m\\}$ 를  만들고 $D_i$라는 개별 문서에 단어장에 해당하는 단어들이 포함되어 있는지를 표시하는 방법이다.\n",
    "\n",
    "$$ \\text{ if word $W_j$ in document $D_i$ }, \\;\\; \\rightarrow x_{ij} = 1 $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn 의 문서 전처리 기능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn 의 feature_extraction.text 서브 패키지는 다음과 같은 문서 전처리용 클래스를 제공한다.\n",
    "\n",
    "* [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html): \n",
    " * 문서 집합으로부터 단어의 수를 세어 카운트 행렬을 만든다.\n",
    "* [`TfidfVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html): \n",
    " * 문서 집합으로부터 단어의 수를 세고 TF-IDF 방식으로 단어의 가중치를 조정한 카운트 행렬을 만든다.\n",
    "* [`HashingVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html): \n",
    " * hashing trick 을 사용하여 빠르게 카운트 행렬을 만든다.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'document': 1,\n",
       " 'first': 2,\n",
       " 'is': 3,\n",
       " 'last': 4,\n",
       " 'one': 5,\n",
       " 'second': 6,\n",
       " 'the': 7,\n",
       " 'third': 8,\n",
       " 'this': 9}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?',    \n",
    "]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 0, 1, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(['This is the second document.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(['Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 처리 옵션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer`는 다양한 인수를 가진다. 그 중 중요한 것들은 다음과 같다.\n",
    "\n",
    "* `stop_words` : 문자열 {‘english’}, 리스트 또는 None (디폴트)\n",
    " * stop words 목록.‘english’이면 영어용 스탑 워드 사용.\n",
    "* `analyzer` : 문자열 {‘word’, ‘char’, ‘char_wb’} 또는 함수\n",
    " * 단어 n-그램, 문자 n-그램, 단어 내의 문자 n-그램 \n",
    "* `tokenizer` : 함수 또는 None (디폴트)\n",
    " * 토큰 생성 함수 .\n",
    "* `token_pattern` : string\n",
    " * 토큰 정의용 정규 표현식 \n",
    "* `ngram_range` : (min_n, max_n) 튜플\n",
    " * n-그램 범위 \n",
    "* `max_df` : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1\n",
    " * 단어장에 포함되기 위한 최대 빈도\n",
    "* `min_df` : 정수 또는 [0.0, 1.0] 사이의 실수.  디폴트 1\n",
    " * 단어장에 포함되기 위한 최소 빈도 \n",
    "* `vocabulary` : 사전이나 리스트\n",
    " * 단어장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Words 는 문서에서 단어장을 생성할 때 무시할 수 있는 단어를 말한다. 보통 영어의 관사나 접속사, 한국어의 조사 등이 여기에 해당한다. `stop_words` 인수로 조절할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'first': 1, 'last': 2, 'one': 3, 'second': 4, 'third': 5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"the\", \"this\"]).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'second': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=\"english\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰은 문서에서 단어장을 생성할 때 하나의 단어가 되는 단위를 말한다. `analyzer`, `tokenizer`, `token_pattern` 등의 인수로 조절할 수 있다.\n",
    "\n",
    "문서를 보고 어떤 언어인지 맞추는 방법은 토큰으로 사용빈도를 보고 맞춘다. 예를 들어 제일 많이 나오는 char를 e로 잡고 그 다음 뭐 인지를 패턴화해서 맞추는 방식으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '.': 1,\n",
       " '?': 2,\n",
       " 'a': 3,\n",
       " 'c': 4,\n",
       " 'd': 5,\n",
       " 'e': 6,\n",
       " 'f': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'l': 10,\n",
       " 'm': 11,\n",
       " 'n': 12,\n",
       " 'o': 13,\n",
       " 'r': 14,\n",
       " 's': 15,\n",
       " 't': 16,\n",
       " 'u': 17}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(analyzer=\"char\").fit(corpus)   #토큰 1개가 vocaburary로 인식. 원래 기본은 word이지만 char가 들어갈 수 있다.\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " '?': 1,\n",
       " 'and': 2,\n",
       " 'document': 3,\n",
       " 'first': 4,\n",
       " 'is': 5,\n",
       " 'last': 6,\n",
       " 'one': 7,\n",
       " 'second': 8,\n",
       " 'the': 9,\n",
       " 'third': 10,\n",
       " 'this': 11}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(tokenizer=nltk.word_tokenize).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0, 'third': 1, 'this': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(token_pattern=\"t\\w+\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-그램"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "n-그램은 단어장 생성에 사용할 토큰의 크기를 결정한다. 1-그램은 토큰 하나만 단어로 사용하며 2-그램은 두 개의 연결된 토큰을 하나의 단어로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and the': 0,\n",
       " 'first document': 1,\n",
       " 'is the': 2,\n",
       " 'is this': 3,\n",
       " 'last document': 4,\n",
       " 'second document': 5,\n",
       " 'second second': 6,\n",
       " 'the first': 7,\n",
       " 'the last': 8,\n",
       " 'the second': 9,\n",
       " 'the third': 10,\n",
       " 'third one': 11,\n",
       " 'this is': 12,\n",
       " 'this the': 13}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2)).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0, 'the third': 1, 'third': 2, 'this': 3, 'this the': 4}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2), token_pattern=\"t\\w+\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 빈도수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`max_df`, `min_df` 인수를 사용하여 문서에서 토큰이 나타난 횟수를 기준으로 단어장을 구성할 수도 있다. 토큰의 빈도가 `max_df`로 지정한 값을 초과 하거나 `min_df`로 지정한 값보다 작은 경우에는 무시한다. 인수 값은 정수인 경우 횟수, 부동소수점인 경우 비중을 뜻한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'document': 0, 'first': 1, 'is': 2, 'this': 3},\n",
       " {'and', 'last', 'one', 'second', 'the', 'third'})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_df=4, min_df=2).fit(corpus)\n",
    "vect.vocabulary_, vect.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1],\n",
       "       [1, 0, 1, 1],\n",
       "       [0, 0, 0, 0],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF(Term Frequency – Inverse Document Frequency) 인코딩은 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소하는 방법이다. \n",
    "\n",
    "\n",
    "구제적으로는 문서 $d$(document)와 단어 $t$ 에 대해 다음과 같이 계산한다.\n",
    "\n",
    "$$ \\text{tf-idf}(d, t) = \\text{tf}(d, t) \\cdot \\text{idf}(d, t) $$\n",
    "\n",
    "\n",
    "여기에서\n",
    "\n",
    "* $\\text{tf}(d, t)$: 단어의 빈도수\n",
    "* $\\text{idf}(d, t)$ : inverse document frequency \n",
    " \n",
    " $$ \\text{idf}(d, t) = \\log \\dfrac{n_d}{1 + \\text{df}(t)} $$\n",
    " \n",
    "* $n_d$ : 전체 문서의 수\n",
    "* $\\text{df}(t)$:  단어 $t$를 가진 문서의 수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1을 더하는 이유는 스무딩 하기 위해서. 너무 커지기 때문에 log를 취해서 스케일링 했음. df(t)가 크면 idf가 작게 된다. idf는 가중치를 축소시키기도 하고 확대시키기도 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.38947624,  0.55775063,  0.4629834 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.32941651,  0.        ,  0.4629834 ],\n",
       "       [ 0.        ,  0.24151532,  0.        ,  0.28709733,  0.        ,\n",
       "         0.        ,  0.85737594,  0.20427211,  0.        ,  0.28709733],\n",
       "       [ 0.55666851,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.55666851,  0.        ,  0.26525553,  0.55666851,  0.        ],\n",
       "       [ 0.        ,  0.38947624,  0.55775063,  0.4629834 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.32941651,  0.        ,  0.4629834 ],\n",
       "       [ 0.        ,  0.45333103,  0.        ,  0.        ,  0.80465933,\n",
       "         0.        ,  0.        ,  0.38342448,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidv = TfidfVectorizer().fit(corpus)\n",
    "tfidv.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer`는 모든 작업을 in-memory 상에서 수행하므로 데이터 양이 커지면 속도가 느려지거나 실행이 불가능해진다. 이 때 \n",
    "`HashingVectorizer`를 사용하면 Hashing Trick을 사용하여 메모리 및 실행 시간을 줄일 수 있다. 하지만 사용 빈도로는 이게 더 잘 안 쓰인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty = fetch_20newsgroups()\n",
    "len(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1787565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time CountVectorizer().fit(twenty.data).transform(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer(n_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.75 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 112863 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time hv.transform(twenty.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석기 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bought': 0,\n",
       " 'buying': 1,\n",
       " 'buys': 2,\n",
       " 'image': 3,\n",
       " 'imagination': 4,\n",
       " 'imagine': 5,\n",
       " 'imaging': 6}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\"imaging\", \"image\", \"imagination\", \"imagine\", \"buys\", \"buying\", \"bought\"]\n",
    "vect = CountVectorizer().fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty = fetch_20newsgroups()\n",
    "docs = twenty.data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'write': 0,\n",
       " 'writer': 1,\n",
       " 'writers': 2,\n",
       " 'writes': 3,\n",
       " 'writing': 4,\n",
       " 'writing_': 5,\n",
       " 'written': 6}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=\"english\", token_pattern=\"wri\\w+\").fit(docs)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'write': 0, 'writer': 1, 'writing_': 2, 'written': 3}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "class StemTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.s = SnowballStemmer('english')\n",
    "        self.t = CountVectorizer(stop_words=\"english\", token_pattern=\"wri\\w+\").build_tokenizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.s.stem(t) for t in self.t(doc)]\n",
    "\n",
    "vect = CountVectorizer(tokenizer=StemTokenizer()).fit(docs)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "JVMNotFoundException",
     "evalue": "No JVM shared library file (jvm.dll) found. Try setting up the JAVA_HOME environment variable properly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJVMNotFoundException\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-257ffd2eb77a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHannanum\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mhannanum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHannanum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://www.datascienceschool.net/download-notebook/708e711429a646818b9dcbb581e0c10a/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_hannanum.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, jvmpath)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjvmpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mjpype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misJVMStarted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[0mjvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_jvm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjvmpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mjhannanumJavaPackage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjpype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJPackage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'kr.lucypark.jhannanum.comm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\konlpy\\jvm.py\u001b[0m in \u001b[0;36minit_jvm\u001b[1;34m(jvmpath)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mclasspath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpathsep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfolder_suffix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mjvmpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjvmpath\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mjpype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetDefaultJVMPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;31m# NOTE: Temporary patch for Issue #76. Erase when possible.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\jpype\\_core.py\u001b[0m in \u001b[0;36mget_default_jvm_path\u001b[1;34m()\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mfinder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinuxJVMFinder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfinder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_jvm_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;31m# Naming compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\jpype\\_jvmfinder.py\u001b[0m in \u001b[0;36mget_jvm_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m                                        \u001b[1;34m\"found. Try setting up the JAVA_HOME \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                                        \u001b[1;34m\"environment variable properly.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                                        .format(self._libfile))\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJVMNotFoundException\u001b[0m: No JVM shared library file (jvm.dll) found. Try setting up the JAVA_HOME environment variable properly."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "\n",
    "req = urllib2.Request(\"https://www.datascienceschool.net/download-notebook/708e711429a646818b9dcbb581e0c10a/\")\n",
    "opener = urllib2.build_opener()\n",
    "f = opener.open(req)\n",
    "json = json.loads(f.read())\n",
    "cell = [\"\\n\".join(c[\"source\"]) for c in json[\"cells\"] if c[\"cell_type\"] == u\"markdown\"]\n",
    "docs = [w for w in hannanum.nouns(\" \".join(cell)) if ((not w[0].isnumeric()) and (w[0] not in string.punctuation))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAECCAYAAAARlssoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkUClICIigUpBREQClYKIiAQqBRERCf4/AlIYTnVGIUQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc6abf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(docs)\n",
    "count = vect.transform(docs).toarray().sum(axis=0)\n",
    "plt.bar(range(len(count)), count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(377ad03459bf, 3),\n",
      " (a181562ac4d8, 2),\n",
      " (a1e4ed2ac65b, 1),\n",
      " (container, 1),\n",
      " (daemon, 1),\n",
      " (docker, 1),\n",
      " (dockeruser, 3),\n",
      " (image, 1),\n",
      " (mingw64, 7),\n",
      " (가동, 8),\n",
      " (가상, 2),\n",
      " (가지, 1),\n",
      " (개념, 1),\n",
      " (경우, 12),\n",
      " (공유, 1),\n",
      " (관련하, 2),\n",
      " (나오기, 1),\n",
      " (내부, 1),\n",
      " (다음, 7),\n",
      " (대표적, 1),\n",
      " (대화형, 3),\n",
      " (데몬, 1),\n",
      " (도커, 32),\n",
      " (때문, 1),\n",
      " (리눅스, 1),\n",
      " (마지막, 1),\n",
      " (마찬가지, 3),\n",
      " (머신, 1),\n",
      " (메모리, 1),\n",
      " (명령, 14),\n",
      " (명령어, 3),\n",
      " (목록, 5),\n",
      " (문자, 1),\n",
      " (문자열, 3),\n",
      " (버튼, 1),\n",
      " (복사, 1),\n",
      " (복수, 1),\n",
      " (복수개의, 1),\n",
      " (사용, 17),\n",
      " (사용자, 1),\n",
      " (사용해, 1),\n",
      " (삭제, 8),\n",
      " (생각, 2),\n",
      " (생성, 1),\n",
      " (수행, 3),\n",
      " (시스템, 2),\n",
      " (시작, 6),\n",
      " (아래, 1),\n",
      " (아이디, 9),\n",
      " (여기, 1),\n",
      " (연결, 1),\n",
      " (오류, 1),\n",
      " (옵션, 4),\n",
      " (외부, 2),\n",
      " (원본, 1),\n",
      " (윈도우즈, 1),\n",
      " (으로, 1),\n",
      " (의미, 1),\n",
      " (의존, 1),\n",
      " (이름, 9),\n",
      " (이미지, 30),\n",
      " (이해, 1),\n",
      " (일부분, 1),\n",
      " (입력, 6),\n",
      " (자동, 1),\n",
      " (자체, 1),\n",
      " (작업, 2),\n",
      " (저장, 2),\n",
      " (정지, 1),\n",
      " (조합, 1),\n",
      " (존재, 4),\n",
      " (중복, 1),\n",
      " (중지, 6),\n",
      " (지정, 3),\n",
      " (첫번, 1),\n",
      " (추가, 2),\n",
      " (출력, 3),\n",
      " (컨테이, 1),\n",
      " (컨테이너, 49),\n",
      " (컨테이너상, 1),\n",
      " (컴퓨터, 4),\n",
      " (콜론, 1),\n",
      " (터미널, 3),\n",
      " (툴박스, 1),\n",
      " (특정, 3),\n",
      " (파일, 1),\n",
      " (포워딩, 1),\n",
      " (포트, 4),\n",
      " (폴더, 1),\n",
      " (표시, 1),\n",
      " (프롬프트, 4),\n",
      " (하나, 1),\n",
      " (해당, 4),\n",
      " (호스트, 5),\n",
      " (호스트간, 2)]\n"
     ]
    }
   ],
   "source": [
    "pprint(zip(vect.get_feature_names(), count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
