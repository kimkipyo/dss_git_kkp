{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 패키지 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 패키지는 Theano 패키지와 마찬가지로 선형 대수 심볼 컴파일러(Symbolic Linear Algebra Compiler)이다. Theano에 비해 분산처리 기능이 강화되어 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow 기본 사용법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theano와 유사하게 TensorFlow도 다음과 같은 과정을 거쳐 사용한다.\n",
    "\n",
    "1. 심볼 변수 정의\n",
    "2. 심볼 관계 정의\n",
    "3. 세션 정의\n",
    "4. 세션 사용\n",
    "\n",
    "세션(Session)은 Theano의 함수와 유사한 역할을 하며 실제 심볼 변수의 관계를 분석하고 값을 계산해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 심볼 변수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow는 `Variable` 명령으로 정의하며 Theano와 달리 각 심볼 변수의 형태를 스칼라, 벡터, 행렬 등으로 명시적으로 정의하지 않는 대신 초기값을 이용해서 형태를 지정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "심볼 변수는 보통 입력한 값들에 대한 정의다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(1.0)\n",
    "y = tf.Variable(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensorflow.python.ops.variables.Variable,\n",
       " tensorflow.python.ops.variables.Variable)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x), type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 심볼 관계 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미 만들어진 심볼 변수에 일반 사칙연산이나 TensorFlow 수학 함수를 사용하여 종속 변수 역할을 하는 심볼 변수를 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u = tf.log(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 세션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow의 세션은 Theano의 함수와 유사한 역할을 하며 실제로 계산 그래프를 생성하고 값을 계산하기 위한 환경을 제공한다. Theano의 함수와 달리 세션 생성과 실행 시작, 종료를 명시해야 한다.\n",
    "\n",
    "* 세션 생성: `Session` 객체 생성\n",
    "* 세션 사용: `run` 메서드\n",
    "* 세션 종료: `close` 메서드. with 문으로 대체하기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세션은 지금 만든 심볼을 뒤쪽에서 쓸 수 있도록 정의한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "1.09861\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(sess.run(z))    \n",
    "    print(sess.run(u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 도 심볼릭 연산에 의한 미분 계산이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[2.0]\n"
     ]
    }
   ],
   "source": [
    "f = x ** 2\n",
    "fx = tf.gradients(f, [x])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(sess.run(f))    \n",
    "    print(sess.run(fx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow에는 Theano와 달리 최적화를 위한 `GradientDescentOptimizer` 등의 클래스가 미리 구현되어 있으므로 사용자가 구현할 필요가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0888317 [-0.43195009] [ 0.82341039]\n",
      "20 0.00217394 [-0.05888844] [ 0.38601264]\n",
      "40 0.000138049 [ 0.05996076] [ 0.32167485]\n",
      "60 8.76638e-06 [ 0.08991029] [ 0.30546197]\n",
      "80 5.56689e-07 [ 0.09745742] [ 0.3013764]\n",
      "100 3.5349e-08 [ 0.0993593] [ 0.30034685]\n",
      "120 2.245e-09 [ 0.09983855] [ 0.30008742]\n",
      "140 1.42601e-10 [ 0.09995931] [ 0.30002204]\n",
      "160 9.05917e-12 [ 0.09998976] [ 0.30000556]\n",
      "180 5.7419e-13 [ 0.09999742] [ 0.30000141]\n",
      "200 3.8245e-14 [ 0.09999934] [ 0.30000037]\n"
     ]
    }
   ],
   "source": [
    "# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\n",
    "x_data = np.random.rand(100).astype(np.float32)\n",
    "y_data = x_data * 0.1 + 0.3   #w가 0.1, b가 0.3\n",
    "\n",
    "# Try to find values for W and b that compute y_data = W * x_data + b\n",
    "# (We know that W should be 0.1 and b 0.3, but Tensorflow will\n",
    "# figure that out for us.)\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))   #w와 b를 우리가 찾아야 하기 때문에 심볼로 정의했다.\n",
    "b = tf.Variable(tf.zeros([1]))     #b는 0으로 이니셜라이즈 했다.\n",
    "y = W * x_data + b    #y도 심볼 변수가 된 것이다.\n",
    "# 여기서 중요한 것은 y와 loss가 핵심이다.\n",
    "# Minimize the mean squared errors.\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))   #MSE나 RSS와 같은 방식의 에러를 정의한 것이다.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)   #데아노 같은 경우에는 최적화 과정을 구하는 것이 어려웠으나\n",
    "                                                     #여기서는 이미 구현된 것 사용\n",
    "train = optimizer.minimize(loss)    #그러면 트리이닝 객체가 만들어진다.\n",
    "\n",
    "# Before starting, initialize the variables.  We will 'run' this first.\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph.\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Fit the line.\n",
    "for step in range(201):   #iteration을 201번 돌렸다. \n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:   #20의 배수가 될 때마다 상태를 확인한다.\n",
    "        print(step, sess.run(loss), sess.run(W), sess.run(b))   #W와 b값이 현재 어디까지 왔는지 확인한다.\n",
    "\n",
    "# Learns best fit is W: [0.1], b: [0.3]\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/dockeruser/data/train-images-idx3-ubyte.gz\n",
      "Extracting /home/dockeruser/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/dockeruser/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/dockeruser/data/t10k-labels-idx1-ubyte.gz\n",
      "Accuracy at step 0: 0.1367\n",
      "Accuracy at step 10: 0.7056\n",
      "Accuracy at step 20: 0.8268\n",
      "Accuracy at step 30: 0.8642\n",
      "Accuracy at step 40: 0.881\n",
      "Accuracy at step 50: 0.8834\n",
      "Accuracy at step 60: 0.8907\n",
      "Accuracy at step 70: 0.8894\n",
      "Accuracy at step 80: 0.8932\n",
      "Accuracy at step 90: 0.8883\n",
      "Adding run metadata for 99\n",
      "Accuracy at step 100: 0.9044\n",
      "Accuracy at step 110: 0.9167\n",
      "Accuracy at step 120: 0.9185\n",
      "Accuracy at step 130: 0.9191\n",
      "Accuracy at step 140: 0.9279\n",
      "Accuracy at step 150: 0.9236\n",
      "Accuracy at step 160: 0.9305\n",
      "Accuracy at step 170: 0.9265\n",
      "Accuracy at step 180: 0.9235\n",
      "Accuracy at step 190: 0.9281\n",
      "Adding run metadata for 199\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import argparse\n",
    "tf.app.flags.FLAGS = tf.python.platform.flags._FlagValues()\n",
    "tf.app.flags._global_parser = argparse.ArgumentParser()\n",
    "\n",
    "#이건 사실 아무 상관이 없는 코드다. argument 주는 부분이다.\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_boolean('fake_data', False, 'If true, uses fake data for unit testing.')\n",
    "flags.DEFINE_integer('max_steps', 200, 'Number of steps to run trainer.')\n",
    "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
    "flags.DEFINE_float('dropout', 0.9, 'Keep probability for training dropout.')\n",
    "flags.DEFINE_string('data_dir', '/home/dockeruser/data', 'Directory for storing data')\n",
    "flags.DEFINE_string('summaries_dir', '/home/dockeruser/logs', 'Summaries directory')\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Import data\n",
    "    mnist = input_data.read_data_sets(FLAGS.data_dir,\n",
    "                                      one_hot=True,\n",
    "                                      fake_data=FLAGS.fake_data)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Create a multilayer model.\n",
    "\n",
    "    # Input placehoolders\n",
    "    with tf.name_scope('input'):\n",
    "        x = tf.placeholder(tf.float32, [None, 784], name='x-input')    #placeholder 나중에 시작할 때 들어가는 값 변경 가능\n",
    "        y_ = tf.placeholder(tf.float32, [None, 10], name='y-input')\n",
    "\n",
    "    with tf.name_scope('input_reshape'):\n",
    "        image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        tf.image_summary('input', image_shaped_input, 10)\n",
    "\n",
    "    # We can't initialize these variables to 0 - the network will get stuck.\n",
    "    def weight_variable(shape):\n",
    "        \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(shape):\n",
    "        \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def variable_summaries(var, name):\n",
    "        \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        tf.scalar_summary('sttdev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "        tf.histogram_summary(name, var)\n",
    "\n",
    "    def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "        \"\"\"Reusable code for making a simple neural net layer.\n",
    "        It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n",
    "        It also sets up name scoping so that the resultant graph is easy to read,\n",
    "        and adds a number of summary ops.\n",
    "        \"\"\"\n",
    "        # Adding a name scope ensures logical grouping of the layers in the graph.\n",
    "        with tf.name_scope(layer_name):\n",
    "            # This Variable will hold the state of the weights for the layer\n",
    "            with tf.name_scope('weights'):\n",
    "                weights = weight_variable([input_dim, output_dim])\n",
    "                variable_summaries(weights, layer_name + '/weights')\n",
    "            with tf.name_scope('biases'):\n",
    "                biases = bias_variable([output_dim])\n",
    "                variable_summaries(biases, layer_name + '/biases')\n",
    "            with tf.name_scope('Wx_plus_b'):\n",
    "                preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "                tf.histogram_summary(layer_name + '/pre_activations', preactivate)\n",
    "            activations = act(preactivate, 'activation')\n",
    "            tf.histogram_summary(layer_name + '/activations', activations)\n",
    "            return activations\n",
    "\n",
    "    hidden1 = nn_layer(x, 784, 500, 'layer1')\n",
    "\n",
    "    with tf.name_scope('dropout'):\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        tf.scalar_summary('dropout_keep_probability', keep_prob)\n",
    "        dropped = tf.nn.dropout(hidden1, keep_prob)\n",
    "\n",
    "    y = nn_layer(dropped, 500, 10, 'layer2', act=tf.nn.softmax)\n",
    "\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        diff = y_ * tf.log(y)\n",
    "        with tf.name_scope('total'):\n",
    "            cross_entropy = -tf.reduce_mean(diff)\n",
    "        tf.scalar_summary('cross entropy', cross_entropy)\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.scalar_summary('accuracy', accuracy)\n",
    "\n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.merge_all_summaries()\n",
    "    train_writer = tf.train.SummaryWriter(FLAGS.summaries_dir + '/train', sess.graph)\n",
    "    test_writer = tf.train.SummaryWriter(FLAGS.summaries_dir + '/test')\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    # Train the model, and also write summaries.\n",
    "    # Every 10th step, measure test-set accuracy, and write test summaries\n",
    "    # All other steps, run train_step on training data, & add training summaries\n",
    "\n",
    "    def feed_dict(train):\n",
    "        \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "        if train or FLAGS.fake_data:\n",
    "            xs, ys = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)\n",
    "            k = FLAGS.dropout\n",
    "        else:\n",
    "            xs, ys = mnist.test.images, mnist.test.labels\n",
    "            k = 1.0\n",
    "        return {x: xs, y_: ys, keep_prob: k}\n",
    "\n",
    "    for i in range(FLAGS.max_steps):\n",
    "        if i % 10 == 0:  # Record summaries and test-set accuracy\n",
    "            summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
    "            test_writer.add_summary(summary, i)\n",
    "            print('Accuracy at step %s: %s' % (i, acc))\n",
    "        else:  # Record train set summaries, and train\n",
    "            if i % 100 == 99:  # Record execution stats\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, _ = sess.run([merged, train_step],\n",
    "                                      feed_dict=feed_dict(True),\n",
    "                                      options=run_options,\n",
    "                                      run_metadata=run_metadata)\n",
    "                train_writer.add_run_metadata(run_metadata, 'step%d' % i)\n",
    "                train_writer.add_summary(summary, i)\n",
    "                print('Adding run metadata for', i)\n",
    "            else:  # Record a summary\n",
    "                summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "                train_writer.add_summary(summary, i)\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    if tf.gfile.Exists(FLAGS.summaries_dir):\n",
    "        tf.gfile.DeleteRecursively(FLAGS.summaries_dir)\n",
    "    tf.gfile.MakeDirs(FLAGS.summaries_dir)\n",
    "    train()\n",
    "    \n",
    "    \n",
    "main(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TensorFlow는 theano와 달리 모형 내부와 결과를 모니터링 할 수 있는 TensorBoard 라는 웹서버를 제공한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard 웹서버는 포트 6006을 사용하므로 만약 도커를 사용하는 경우에는 다음과 같이 포트를 열고 실행해야 한다.\n",
    "\n",
    "```\n",
    "docker run --name rpython -it -p 8888:8888 -p 6006:6006 datascienceschool/rpython\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard를 가동하기 위해서는 콘솔에서 다음과 같이 로그 디렉토리를 설정하여 실행한다.\n",
    "\n",
    "```\n",
    "$ tensorboard --logdir=/home/dockeruser/logs &\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 실행한 다음에는 다음 주소로 연결하면 TensorBoard 화면을 볼 수 있다.\n",
    "\n",
    "http://192.168.99.100:6006\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://datascienceschool.net/upfiles/cb3cc08fcafd437e873ca710a23d7c0a.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow는 Theano처럼 `theano.printing.pydotprint` 명령을 지원하지 않는 대신 TensorBoard를 통해 그래프를 보여줄 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그래프 만드는 명령어 볼 수 없다. 그래프 보려면 텐서보드에서 보아야 한다.\n",
    "- function을 정의하는 것 대신에 세션을 정의해야 한다. function이 없기 때문에 call을 할 수가 없다.\n",
    "- 함수와 세션의 차이는? 실행 컨테이너라고 부르는 것을 teano에서는 바깥에서 정의를 했기 때문에 함수를 쓰지 않았다. 하지만 함수를 정의할 때는 인풋아웃풋의 관계? 때문은 아니었다. 세션에서는 그럴 필요가 없다. 관계가 된 아웃풋을 구하면 끝이다. 필요할 때 쓰는 것이다. 세션할 때는 이니셜라이즈(초기화) 항상 해주어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MNIST 예제\n",
    " * 노트북에서 실행 원래 안 되는데 코드 좀 바꾸어서 실행 되게 만들었다.\n",
    " * 코드 실행하고 TensorBoard에 들어가면 내용이 뜬다.\n",
    " * cross entropy를 cost function으로 썼다. 줄고 있는가? 좀 더 천천히 내려가는 특성이 있다. 계속 내려간다. 어느정도 내리면 별 의미가 없다.\n",
    " * dropout? 이건 이따가 설명\n",
    " * 28X28 들어가는 것이라서 몇 백개 되는 weight 어렵다. 그래서 max값이나 mean값의 움직임만 보여주는 것이다. layer가 2개이기 때문에 나누어졌다. 맨 밑에는 stadard deviation이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
